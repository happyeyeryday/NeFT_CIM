import subprocess
import argparse
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import time
import sys
import threading
import signal
import psutil

def run_experiment(method, dataset, mode, var1=None, var2=None, kl_weight=None, num='1', mark='1.0'):
    """è¿è¡Œå•ä¸ªå®éªŒ"""
    if method == 'bnn':
        cmd = ['python', 'main_bnn.py', '--mode', mode, '--dataset', dataset, 
               '--num', num, '--mark', mark]
        if kl_weight is not None:
            cmd.extend(['--kl_weight', str(kl_weight)])
    elif method == 'bnn_mean':
        # ç‰¹æ®Šå¤„ç†ï¼šBNNå‡å€¼æƒé‡ä½¿ç”¨æ ‡å‡†æµ‹è¯•
        cmd = ['python', 'res18_main.py', '--mode', mode, '--type', 'base', 
               '--dataset', dataset, '--device', 'RRAM1', '--num', num, '--mark', mark]
        cmd.extend(['--bnn_mean', 'True'])
        if var1 is not None:
            cmd.extend(['--var1', str(var1)])
        if var2 is not None:
            cmd.extend(['--var2', str(var2)])
    else:
        cmd = ['python', 'res18_main.py', '--mode', mode, '--type', method, 
               '--dataset', dataset, '--device', 'RRAM1', '--num', num, '--mark', mark]
        if var1 is not None:
            cmd.extend(['--var1', str(var1)])
        if var2 is not None:
            cmd.extend(['--var2', str(var2)])
    
    print(f"ğŸš€ Running command: {' '.join(cmd)}")
    
    # ğŸ”§ ä¿®å¤ï¼šä¸ºæµ‹è¯•æ¨¡å¼è®¾ç½®æ›´çŸ­è¶…æ—¶
    if mode in ['test', 'test_mean']:
        timeout = 1800  # æµ‹è¯•é˜¶æ®µ30åˆ†é’Ÿè¶…æ—¶ï¼ˆè€ƒè™‘MCæ¨ç†ï¼‰
        print(f"â±ï¸  Starting subprocess with timeout={timeout}s (5 minutes for test)...")
    else:
        timeout = 7200  # è®­ç»ƒé˜¶æ®µ2å°æ—¶è¶…æ—¶
        print(f"â±ï¸  Starting subprocess with timeout={timeout}s (2 hours for train)...")
    
    try:
        # ä½¿ç”¨Popenä»¥ä¾¿æ›´å¥½åœ°æ§åˆ¶è¿›ç¨‹
        process = subprocess.Popen(
            cmd, 
            stdout=subprocess.PIPE, 
            stderr=subprocess.PIPE,
            universal_newlines=True
        )
        
        try:
            # ç­‰å¾…è¿›ç¨‹å®Œæˆï¼Œå¸¦è¶…æ—¶
            stdout, stderr = process.communicate(timeout=timeout)
            returncode = process.returncode
        except subprocess.TimeoutExpired:
            print(f"â° Process timed out after {timeout} seconds, terminating...")
            
            # å¼ºåˆ¶ç»ˆæ­¢è¿›ç¨‹åŠå…¶å­è¿›ç¨‹
            try:
                parent = psutil.Process(process.pid)
                children = parent.children(recursive=True)
                for child in children:
                    child.terminate()
                parent.terminate()
                
                # ç­‰å¾…è¿›ç¨‹ç»ˆæ­¢
                process.wait(timeout=5)
            except:
                # å¦‚æœæ¸©å’Œç»ˆæ­¢å¤±è´¥ï¼Œå¼ºåˆ¶æ€æ­»
                process.kill()
                process.wait()
            
            return None, "Timeout", -1
        
        # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•è¾“å‡º
        print(f"âœ… Subprocess completed!")
        print(f"ğŸ“Š Return code: {returncode}")
        print(f"ğŸ“ STDOUT length: {len(stdout) if stdout else 0} characters")
        print(f"ğŸ“ STDERR length: {len(stderr) if stderr else 0} characters")
        
        if returncode == 0:
            print(f"ğŸ‰ Command executed successfully!")
        else:
            print(f"âŒ Command failed with return code: {returncode}")
        
        # æ˜¾ç¤ºè¾“å‡ºé¢„è§ˆ
        if stdout:
            print(f"ğŸ“¤ STDOUT preview (first 300 chars):")
            print(f"   {stdout[:300]}...")
        else:
            print(f"ğŸ“¤ STDOUT: Empty or None")
            
        if stderr:
            print(f"âš ï¸  STDERR preview (first 300 chars):")
            print(f"   {stderr[:300]}...")
        else:
            print(f"âš ï¸  STDERR: Empty or None")
        
        return stdout, stderr, returncode
        
    except Exception as e:
        print(f"ğŸ’¥ Unexpected error occurred: {str(e)}")
        print(f"ğŸ’¥ Error type: {type(e).__name__}")
        return None, f"Error: {str(e)}", -1


def parse_accuracy(output):
    """ä»è¾“å‡ºä¸­è§£æå‡†ç¡®ç‡"""
    if not output:
        return None
    
    lines = output.split('\n')
    
    # å°è¯•å¤šç§æ¨¡å¼åŒ¹é…å‡†ç¡®ç‡
    patterns = [
        r'accuracy[:\s=]+(\d+\.\d+)%?',  # "accuracy: 85.5%" æˆ– "accuracy = 85.5"
        r'acc[:\s=]+(\d+\.\d+)%?',      # "acc: 85.5%" æˆ– "acc = 85.5"  
        r'test accuracy[:\s=]+(\d+\.\d+)%?',  # "test accuracy: 85.5%"
        r'final accuracy[:\s=]+(\d+\.\d+)%?',  # "final accuracy: 85.5%"
    ]
    
    import re
    for line in lines:
        line_lower = line.lower()
        for pattern in patterns:
            match = re.search(pattern, line_lower)
            if match:
                try:
                    return float(match.group(1))
                except ValueError:
                    continue
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æå–æ‰€æœ‰æ•°å­—ï¼Œé€‰æ‹©æœ€å¯èƒ½çš„å‡†ç¡®ç‡å€¼
    for line in lines:
        if any(keyword in line.lower() for keyword in ['accuracy', 'acc', 'test']):
            numbers = re.findall(r'\d+\.\d+', line)
            for num_str in numbers:
                num = float(num_str)
                if 0 <= num <= 100:  # å‡†ç¡®ç‡åº”è¯¥åœ¨0-100ä¹‹é—´
                    return num
    
    return None


def compare_methods(dataset='cifar10', num_runs=1):
    """æ¯”è¾ƒä¸åŒæ–¹æ³•çš„æ€§èƒ½"""
    # ä»ä½œè€…åŸè®ºæ–‡è·å–çš„æœ€ä¼˜å‚æ•°ï¼ˆå¯æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼‰
    methods = {
        'base': {'var1': 0.0, 'var2': 0.0},
        'ovf': {'var1': 0.1, 'var2': 0.05},
        'irs': {'var1': 0.1, 'var2': 0.05},  
        'bnn': {'kl_weight': 1e-5}  # ä½¿ç”¨æ›´æ ‡å‡†çš„KLæƒé‡
    }
    
    results = {}
    
    for method_name, params in methods.items():
        print(f"\n{'='*50}")
        print(f"Running {method_name.upper()} method")
        print(f"{'='*50}")
        accuracies = []
        
        for run in range(num_runs):
            print(f"\nRun {run + 1}/{num_runs} for {method_name.upper()}")
            success = False
            
            # ç»Ÿä¸€çš„numå‚æ•° - ç¡®ä¿è®­ç»ƒå’Œæµ‹è¯•ä½¿ç”¨ç›¸åŒå€¼
            run_num = str(run + 1)
            
            # è®­ç»ƒé˜¶æ®µ
            print(f"  Training {method_name}...")
            if method_name == 'bnn':
                stdout, stderr, returncode = run_experiment(
                    method_name, dataset, 'train', 
                    kl_weight=params['kl_weight'], 
                    num=run_num, mark='1.0'  # ä½¿ç”¨ç»Ÿä¸€çš„run_num
                )
            else:
                stdout, stderr, returncode = run_experiment(
                    method_name, dataset, 'train',
                    var1=params['var1'], var2=params['var2'], 
                    num=run_num, mark='1.0'  # ä½¿ç”¨ç»Ÿä¸€çš„run_num
                )
            
            # ä¿®å¤ï¼šåŸºäºreturncodeåˆ¤æ–­æˆåŠŸ
            if returncode == 0 and stderr != "Timeout":
                print(f"  âœ“ Training completed for {method_name}")
                
                # BNNéœ€è¦é¢å¤–çš„æƒé‡æå–æ­¥éª¤
                if method_name == 'bnn':
                    print(f"  Extracting mean weights from BNN...")
                    stdout_extract, stderr_extract, returncode_extract = run_experiment(
                        method_name, dataset, 'extract_mean',
                        num=run_num, mark='1.0'  # ä½¿ç”¨ç»Ÿä¸€çš„run_num
                    )
                    if returncode_extract == 0 and stderr_extract != "Timeout":
                        print(f"  âœ“ Mean weights extracted successfully")
                    else:
                        print(f"  âœ— Failed to extract mean weights")
                        continue
                
                # æµ‹è¯•é˜¶æ®µ - ä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„numå‚æ•°
                print(f"  Testing {method_name}...")
                if method_name == 'bnn':
                    # BNNä½¿ç”¨æå–çš„å‡å€¼æƒé‡è¿›è¡Œæ ‡å‡†æµ‹è¯•
                    stdout, stderr, returncode = run_experiment(
                        'bnn', dataset, 'test_mean',
                        kl_weight=params['kl_weight'],  # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨kl_weightå‚æ•°
                        num=run_num, mark='1.0'  # ğŸ”§ å…³é”®ä¿®å¤ï¼šä½¿ç”¨ç›¸åŒçš„run_num
                    )
                else:
                    # å…¶ä»–æ–¹æ³•æ­£å¸¸æµ‹è¯• - ä½¿ç”¨ç›¸åŒçš„numå‚æ•°
                    stdout, stderr, returncode = run_experiment(
                        method_name, dataset, 'test',
                        var1=params['var1'], var2=params['var2'], 
                        num=run_num, mark='1.0'  # ğŸ”§ å…³é”®ä¿®å¤ï¼šä½¿ç”¨ç›¸åŒçš„run_num
                    )
                
                # è§£ææµ‹è¯•ç»“æœ
                if returncode == 0 and stdout:
                    acc = parse_accuracy(stdout)
                    if acc is not None:
                        accuracies.append(acc)
                        print(f"  âœ“ {method_name} Run {run+1} Accuracy: {acc:.2f}%")
                        success = True
                    else:
                        print(f"  âœ— Failed to parse accuracy from output")
                        print(f"  Output preview: {stdout[:200]}...")
                else:
                    print(f"  âœ— No output from test or test failed")
            else:
                print(f"  âœ— Training failed for {method_name}")
                print(f"  Return code: {returncode}")
                if stderr and stderr != "Timeout":
                    print(f"  Error: {stderr[:200]}...")
            
            if not success:
                print(f"  âš ï¸  Run {run+1} failed, skipping...")
        
        # ä¿å­˜ç»“æœ
        if accuracies:
            results[method_name] = {
                'mean': np.mean(accuracies),
                'std': np.std(accuracies),
                'values': accuracies
            }
            print(f"\n{method_name.upper()} Final Results:")
            print(f"  Mean Accuracy: {np.mean(accuracies):.2f}%")
            print(f"  Std Deviation: {np.std(accuracies):.2f}%")
            print(f"  All Runs: {accuracies}")
        else:
            print(f"\nâš ï¸  No successful runs for {method_name.upper()}")
    
    return results


def plot_comparison(results, dataset):
    """ç»˜åˆ¶æ¯”è¾ƒç»“æœ"""
    if not results:
        print("No results to plot")
        return None
    
    methods = list(results.keys())
    means = [results[m]['mean'] for m in methods]
    stds = [results[m]['std'] for m in methods]
    
    # è®¾ç½®é¢œè‰²æ–¹æ¡ˆ
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'][:len(methods)]
    
    # åˆ›å»ºæŸ±çŠ¶å›¾
    plt.figure(figsize=(12, 8))
    bars = plt.bar(methods, means, yerr=stds, capsize=5, alpha=0.8, color=colors)
    
    # ç¾åŒ–å›¾è¡¨
    plt.title(f'Robustness Method Comparison on {dataset.upper()}', fontsize=18, fontweight='bold')
    plt.ylabel('Test Accuracy (%)', fontsize=16)
    plt.xlabel('Methods', fontsize=16)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (method, mean, std) in enumerate(zip(methods, means, stds)):
        plt.text(i, mean + std + 1, f'{mean:.2f}Â±{std:.2f}%', 
                ha='center', va='bottom', fontweight='bold', fontsize=12)
    
    # æ·»åŠ ç½‘æ ¼å’Œæ ·å¼
    plt.grid(axis='y', alpha=0.3, linestyle='--')
    plt.ylim(0, max(means) + max(stds) + 10)
    
    # è°ƒæ•´xè½´æ ‡ç­¾
    method_labels = [m.upper() for m in methods]
    plt.xticks(range(len(methods)), method_labels, fontsize=14)
    plt.yticks(fontsize=12)
    
    plt.tight_layout()
    plt.savefig(f'comparison_{dataset}.png', dpi=300, bbox_inches='tight')
    plt.savefig(f'comparison_{dataset}.pdf', bbox_inches='tight')
    
    try:
        plt.show()
    except:
        print("Cannot display plot (no GUI), but saved to file")
    
    # åˆ›å»ºè¯¦ç»†ç»“æœè¡¨
    df_data = []
    for method, result in results.items():
        df_data.append({
            'Method': method.upper(),
            'Mean Accuracy (%)': f"{result['mean']:.2f}",
            'Std Deviation (%)': f"{result['std']:.2f}",
            'Best Run (%)': f"{max(result['values']):.2f}",
            'Worst Run (%)': f"{min(result['values']):.2f}",
            'Number of Runs': len(result['values'])
        })
    
    df = pd.DataFrame(df_data)
    print("\n" + "="*80)
    print("DETAILED COMPARISON RESULTS")
    print("="*80)
    print(df.to_string(index=False))
    print("="*80)
    
    return df


def format_time(seconds):
    """æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    seconds = seconds % 60
    return f"{hours}h {minutes}m {seconds:.1f}s"


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Compare BNN vs Traditional Methods')
    parser.add_argument('--dataset', type=str, default='cifar10', 
                        choices=['mnist', 'cifar10', 'cifar100', 'tiny'],
                        help='Dataset to use for comparison')
    parser.add_argument('--runs', type=int, default=3, 
                        help='Number of runs for each method (default: 3)')
    parser.add_argument('--timeout', type=int, default=7200,
                        help='Timeout per experiment in seconds (default: 2 hours)')
    
    args = parser.parse_args()
    
    print("="*80)
    print("NEURAL NETWORK ROBUSTNESS COMPARISON EXPERIMENT")
    print("="*80)
    print(f"Dataset: {args.dataset.upper()}")
    print(f"Runs per method: {args.runs}")
    print(f"Methods: BASE, OVF, IRS, BNN")
    print(f"Timeout per run: {format_time(args.timeout)}")
    print("="*80)
    
    start_time = time.time()
    
    # è¿è¡Œæ¯”è¾ƒå®éªŒ
    print("\nStarting comparison experiment...")
    results = compare_methods(args.dataset, args.runs)
    
    # ç»˜åˆ¶å’Œä¿å­˜ç»“æœ
    if results:
        print(f"\nâœ“ Experiment completed! Processing results...")
        df = plot_comparison(results, args.dataset)
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        csv_filename = f'comparison_results_{args.dataset}_{timestamp}.csv'
        df.to_csv(csv_filename, index=False)
        
        print(f"\nğŸ“Š Results saved to:")
        print(f"  - {csv_filename}")
        print(f"  - comparison_{args.dataset}.png")
        print(f"  - comparison_{args.dataset}.pdf")
        
        # æ‰“å°æœ€ç»ˆæ€»ç»“
        print(f"\nğŸ† FINAL RANKING (by mean accuracy):")
        sorted_results = sorted(results.items(), key=lambda x: x[1]['mean'], reverse=True)
        for i, (method, result) in enumerate(sorted_results, 1):
            print(f"  {i}. {method.upper()}: {result['mean']:.2f}% Â± {result['std']:.2f}%")
    else:
        print("\nâŒ No results obtained. Please check your setup and try again.")
        print("\nTroubleshooting tips:")
        print("1. Ensure all required scripts (main_bnn.py, res18_main.py) are present")
        print("2. Check if models are properly saved after training")
        print("3. Verify dataset availability")
        print("4. Consider reducing the number of runs or using a smaller dataset")
    
    total_time = time.time() - start_time
    print(f"\nâ±ï¸  Total experiment time: {format_time(total_time)}")